{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jupyter Notebook Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Points\n",
    "* There are a number of Python profiling tools that can be used to profile code in a Jupyter notebook.\n",
    "* `timeit`, `prun` and `lpun` can help you identify performance bottlenecks in your code\n",
    "* `memit` and `mprun` can help you find memory issues in your code\n",
    "* Many Python profiling tools can be used to profile third-party dependencies such as pandas and numpy\n",
    "* Built-in pandas and numpy functions are optimized for performance on fixed-type arrays. Use them when possible.\n",
    "* The `dis` module can be used to inspect Python bytecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic Commands\n",
    "\n",
    "Magic commands are shortcuts to help solve problems when working in notebooks.\n",
    "\n",
    "* Magic commands can work on a line or a cell\n",
    "* Examples - load external code, formatting \n",
    "* Use %magic for help\n",
    "\n",
    "For detailed information on magic commands, see https://ipython.readthedocs.io/en/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lsmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%markdown\n",
    "### The %%markdown cell magic displays text as markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<b>Bold</b>\n",
    "<script>alert('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%loadpy file.py OR URL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Magic\n",
    "There are several magic commands that are useful for profiling code in a Jupyter notebook:\n",
    "\n",
    "* %time: Time the execution of a single statement\n",
    "* %timeit: Time repeated execution of a single statement for more accuracy\n",
    "* %prun: Run code with the profiler\n",
    "* %lprun: Run code with the line-by-line profiler\n",
    "* %memit: Measure the memory use of a single statement\n",
    "* %mprun: Run code with the line-by-line memory profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`time` uses Python's `time` module which is not specifically designed for performance testing.  It allows you to get a rough estimate of execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIG_NUMBER = 100000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some code to generate a list of odd numbers from 1 to BIG_NUMBER. A programmer familiar with Java or C might write the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_slow_list = []               # intialize an empty list\n",
    "for i in range(BIG_NUMBER):     # use range() to loop from 0 to 999999\n",
    "    if i % 2 == 1:              # if the number is odd...\n",
    "        my_slow_list.append(i)  # add it to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how this code performs. Below, the presence of `%%` indicates a cell magic. Cell magic applies to an entire cell. We can use the `%%time` cell magic to time our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "my_slow_list = []               # intialize an empty list\n",
    "for i in range(BIG_NUMBER):     # use range() to loop from 0 to 999999\n",
    "    if i % 2 == 1:              # if the number is odd...\n",
    "        my_slow_list.append(i)  # add it to the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, we can write the same code using a list comprehension. From the Python docs, \"List comprehensions provide a concise way to create lists. Common applications are to make new lists where each element is the result of some operations applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, the presence of `%` indicates a line magic. Line magic applies to a single line. We can use the `%time` line magic to time the list comprehension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time my_list = [i for i in range(BIG_NUMBER) if i % 2 == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do even better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time my_list = list(range(1,BIG_NUMBER,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time module works well but it has an issue. The %time magic only runs a single execution of the code. It would be nice to see an average of multiple executions so we can determine the true expected performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## timeit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`timeit` is specifically designed execute multiple iterations of a line, function or block of code. It calculates performance more accurately than the time module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -r 5 -n 100\n",
    "# -r: numer of runs to execute\n",
    "# -n: number of iterations per run\n",
    "\n",
    "my_slow_list = []\n",
    "for i in range(BIG_NUMBER):\n",
    "    if i % 2 == 1:\n",
    "        my_slow_list.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 5 -n 100 [i for i in range(BIG_NUMBER) if i % 2 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 5 -n 100 list(range(1,BIG_NUMBER,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prun\n",
    "\n",
    "`prun` executes the Python code profiler and can provide various run-time metrics as well as the execution path information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%prun -s tottime\n",
    "# -s sort descending by given column key\n",
    "\n",
    "my_slow_list = []\n",
    "for i in range(BIG_NUMBER):\n",
    "    if i % 2 == 1:\n",
    "        my_slow_list.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation of the profiler output can be found at https://docs.python.org/3/library/profile.html\n",
    "\n",
    "* ncalls - for the number of calls.\n",
    "* tottime - for the total time spent in the given function (and excluding time made in calls to sub-functions)\n",
    "* percall - is the quotient of tottime divided by ncalls\n",
    "* cumtime - is the cumulative time spent in this and all subfunctions (from invocation till exit). This figure is accurate even for recursive functions.\n",
    "* percall - is the quotient of cumtime divided by primitive calls\n",
    "* filename:lineno(function) - provides the respective data of each function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The filename `<string>:#(<module>)` is the entry point of the script that was invoked (the top-level code that invoked the for-loop). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -s tottime my_list = [i for i in range(BIG_NUMBER) if i % 2 == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -s tottime my_list = list(range(1,BIG_NUMBER,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning\n",
    "Profiling tools use different methods for profiling. There's a good description of the various techniques and limitations at https://docs.python.org/3/library/profile.html#what-is-deterministic-profiling. When using profilers, it's best not to compare results across profiling tools as it can result in an apples-to-oranges comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lprun\n",
    "\n",
    "The output from `prun` is useful but can be difficult to read. `lprun` is a line profiler that provides profiling information for each line of code. `lprun` only works on functions. You must have the `line_profiler package` installed to run `lprun`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the line_profiler extension\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rewrite our code as a function so we can run `lprun` against it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_list(my_slow_list):\n",
    "    my_slow_list = []\n",
    "    for i in range(BIG_NUMBER):\n",
    "        if i % 2 == 1:\n",
    "            my_slow_list.append(i)\n",
    "    return my_slow_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_slow_list = []\n",
    "%lprun -f slow_list slow_list(my_slow_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An explanation of the output can be found at https://nyu-cds.github.io/python-performance-tuning/03-line_profiler/\n",
    "\n",
    "* Timer unit - gives the conversion factor to seconds for time information (1e-06s = 1 microsecond)\n",
    "* Line # - The line number in the code\n",
    "* Hits - The number of times that line was executed\n",
    "* Time - The total amount of time spent executing the line in the timer’s units\n",
    "* Per Hit - The average amount of time spent executing the line once in the timer’s unit\n",
    "* % Time - The percentage of time spent on that line relative to the total amount of recorded time spent in the function\n",
    "* Line Contents - The actual source code of the line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a data set using pandas as we would in a typical data science project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%timeit -r 1 -n 5 pd.read_csv(\"../../data/nyc_taxi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`prun` can be used to profile code in libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -l - limit results to the first 20 lines\n",
    "\n",
    "%prun -l 20 -s tottime pd.read_csv(\"../../data/nyc_taxi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function calls includes any recursive calls. Primitive calls only include non-recursive function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/nyc_taxi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably don't want the `timestamp` and `value` columns parsed as object or int64. We know that `value` is a positive count of the number of taxi riders so we can parse it as a `uint` (unsigned integer) to save memory. Timestamp is a timestamp, so we should parse that as a `datetime`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the min/max values of `value` to see which data type is most appropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['value'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['value'].min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the min value of `value` is 8 and the max value is 39197, we can store it as a `uint16` (0-65535) to save memory. We can also parse the timestamp field so it's stored as a `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "column_types = {\n",
    "    'value': 'uint16',\n",
    "    }\n",
    "\n",
    "dateparse = lambda timestamp: datetime.strptime(timestamp, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "%timeit -r 1 -n 5 pd.read_csv(\"../../data/nyc_taxi.csv\", dtype=column_types, parse_dates=['timestamp'], date_parser=dateparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, loading the file takes significantly longer. It takes more than 100ms to load the file. Previously, it took less than 10ms. But..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../../data/nyc_taxi.csv\", dtype=column_types, parse_dates=['timestamp'], date_parser=dateparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info(memory_usage='deep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saved roughly 700K (87% memory savings!) in memory by using more appropriate data types. Did the `uint` or `datetime` parsing change affect the runtime? We can use `prun` to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%prun -l 20 -s tottime pd.read_csv(\"../../data/nyc_taxi.csv\", dtype=column_types, parse_dates=['timestamp'], date_parser=dateparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `strptime` function looks to be taking the most time. Let's remove it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 1 -n 5 pd.read_csv(\"../../data/nyc_taxi.csv\", dtype=column_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, date parsing was the culprit because the runtime is now closer to the original runtime. We can take our analysis one step further and dig into the pandas source code, but that's probably not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f pd.read_csv pd.read_csv(\"../../data/nyc_taxi.csv\", dtype=column_types, parse_dates=['timestamp'], date_parser=dateparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the pandas `read_csv()` function delegates to `_read()` so there's not a lot to see here... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Pandas Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we need to sum the values in the \"value\" column. If we're new to pandas, we might try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_values(df):\n",
    "    sum_val = 0                             # intialize the return value to zero\n",
    "    for i in range(len(df)):                # loop over all rows in the data frame\n",
    "        sum_val = df[\"value\"][i] + sum_val  # calculate a running sum\n",
    "    return sum_val                          # return the sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_values(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 1 -n 5 sum_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f sum_values sum_values(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a better way to do this. Let's try using the pandas `sum()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_values_faster(df):\n",
    "    return df[\"value\"].sum()   # use the dataframe sum() method to calculate the sum and return in a single step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum_values_faster(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -r 1 -n 5 sum_values_faster(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%lprun -f sum_values_faster sum_values_faster(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Iterating over numpy arrays or pandas data strutures is almost always slower than using built-in functions**\n",
    "\n",
    "Pandas and numpy are optimized for working with fixed-type arrays so pandas/numpy function run-times are usually better than custom code. A numpy array effectively uses a single pointer to a contiguous block of data instead of using individual pointers to non-contiguous memory locations. Fixed-type arrays lack the flexibility of Python lists but are much more efficient for storing and manipulating data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## memit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simlar to `timeit`, `memit` allows you to profile memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_my_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%memit get_my_data(\"../../data/nyc_taxi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increment shows us that the `get_my_data` function increased memory usage by 4MB. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mprun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mprun` is similar to `lprun`. It performs line-by-line memory profiling. It only works on code that's been loaded as a module so we need to write the function to a file prior to profiling. We can use the `%file` magic to write the file before loading the module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file csv_loader.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_my_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the function from the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv_loader import get_my_data\n",
    "%mprun -f get_my_data get_my_data(\"../../data/nyc_taxi.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the pd.read_csv call causes memory usage to increase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Profiling Example\n",
    "\n",
    "An example from https://pythonspeed.com/articles/minimizing-copying/\n",
    "\n",
    "Python handles memory management for us. This is great, but it can be tricky to understand how much memory a specific function uses. \n",
    "\n",
    "The `normalize` function below takes a 1-dimensional numpy array and returns a normalized array to the caller. Take a guess at how much memory the function below uses?\n",
    "\n",
    "A reasonable guess is that it uses slightly more memory that the size of `np_array` - the slight increase being due to the creation of the `low` and `high` local variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file memory_hog.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def normalize(np_array):\n",
    "    low = np_array.min()                         \n",
    "    high = np_array.max()                        \n",
    "    return ( np_array - low ) / ( high - low )   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_hog import normalize\n",
    "import numpy as np\n",
    "\n",
    "%mprun -f normalize normalize(np.arange(1, 6000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, the `normalize` function uses more memory than expected. `mprun` shows us that Python creates a temporary copy of `np_array` prior to returning it to the caller. We know this because the Increment column shows that memory usage increased on line #7, the return statement. Based purely on the function code, we might not expect that to happen. Is there another way to handle this situation and  elminates the need for a temporary object in memory?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use numpy's in-place operations to perform operations on the existing array rather than creating a temporary array. This is another example of how pandas/numpy operations can be faster and more efficient than custom code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%file improved_memory_hog.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def normalize(np_array):\n",
    "    low = np_array.min()\n",
    "    high = np_array.max()\n",
    "    np_array -= low\n",
    "    np_array /= ( high - low )\n",
    "    return np_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from improved_memory_hog import normalize as normalize_improved\n",
    "import numpy as np\n",
    "\n",
    "%mprun -f normalize_improved normalize_improved(np.arange(1, 6000000, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Another Warning\n",
    "Although the new `normalize` function doesn't use as much memory, it mutates the function argument `np_array`. This can be problemmatic unless callers are expecting this behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep in the Weeds - Inspecting Python Bytecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you really want to dig into performance, the `dis` module gives you the ability to inspect the Python bytecode generated by a function. When you run a Python program, the Python interpreter generates bytecode which is executed by the Python runtime (CPython in our case). The bytecode can help reveal why certain code runs faster/slower than \"equivalent\" code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the bytecode from our earlier example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_list():\n",
    "    my_slow_list = []               # intialize an empty list\n",
    "    for i in range(BIG_NUMBER):     # use range() to loop from 0 to 999999\n",
    "        if i % 2 == 1:              # if the number is odd...\n",
    "            my_slow_list.append(i)  # add it to the list\n",
    "    return my_slow_list\n",
    "\n",
    "dis.dis(slow_list_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_comp():\n",
    "    return [i for i in range(BIG_NUMBER) if i % 2 == 1]\n",
    "\n",
    "dis.dis(list_comp_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_list():\n",
    "    return list(range(1,BIG_NUMBER,2))\n",
    "\n",
    "dis.dis(fast_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's easy to see how differences in bytecode can lead to different performance characteristics. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
